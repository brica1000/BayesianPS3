{% extends 'stats/index.html' %}
{% block content %}

<div class="paper-section">
  <h3>I. Introduction</h3>
    <p class="paper-text">
      This project was born out of a curiosity about people's experience using <a target="_blank" href="https://www.couchsurfing.com">CouchSurfing.com</a>, a platform for meeting people.
      As the internet and social media play an ever-growing role in our lives, it is important to understand
      how these interactions are playing out;
      my childhood was filled with warnings about the lurking dangers online, yet, more and more of us
      are turning to the internet for every service imaginable.
      I would like to use people's experiences on CouchSurfing to see what we can learn about how
      things are going as we increasingly
      mix our on and offline lives.
      <br><br>
      The plan is to write a program to scrape some simple data from the all hosts in a
      particular city (at least all those who are active), I choose Helsinki to start with,
      and then try and answer the simple question - which factors make it more likely that
      someone will have a bad experience?
      In answering this question, I have elected to do my analysis within the Bayesian
      framework based on a linear probit model.  This has been shown to be efficient, as well as have
      all the advantages that come by working in the Bayesian realm.<sup>1</sup>
      The rest of the paper will proceed as follows; section II. will outline the
      data set and how it was collected,
      in III. I will go into the details and theory of the model and algorithm used,
      in IV. Then, I will present my results, which will be discussed in
      V.  Finally in section
      VI. one can find my sources as well as links to my code.
      <br><br>
      As one final note, doubtless the reader has noticed that this paper is presented online - this is for two reasons,
      one, it was simply fun, and two, it allows the reader to interact with my code
      and generate their own graphs.  This is a highly desirable trait within the Bayesian framework, as well as
      in any research because I believe it enhances the ease
      with which one could replicate my inquiry.  Our scientific community has a
      dreadful track record of publishing
      results that are not replicable.
    </p>
</div>

<div class="paper-section">
    <h3>II. The Data</h3>
      <p class="paper-text">
        The data set analyzed below is only modest start on the path to understanding the interactions
        born from online networks, however, it should serve to illustrate our procedure and motivate future
        research.  To collect
        the data was
         as simple as going through every host listed in Helsinki
        and recording information such as age, sex, location, how long they have been a member, how long
        their profile is and other such measures easily available.  As this would be rather tedious to complete
        by hand, I wrote a program, which has the great advantage that it is simple to gather more, or to
        extend and create more refined data sets.  I have selected, from everything collected, age (beta 1), profile length (beta2),
        how long a
        user has been a member (beta 3), a dummy variable for males (beta 4), and a constant (beta 0)
        for my data matrix X.  The dependent variable y is a binary outcome
        that is 1 if a user's profile has no neutral/negative reviews, and 0 if there are one or more.
        <br><br>
        Let's go over some summary statistics that motivate our model.  First, we see that
        the average age is 31 for those
        having good experiences, and 34 for bad.  Profile length is 1670 characters for those having good
        experience compared to 2293, somewhat surprisingly.  Also members having good experiences have on average
        been a member for 6 years compared to 8 for those having bad experiences.  Naturally the sample is evenly
        split between the sexes, except there are 60% males in the sample having bad experiences.
        But before moving on to the procedure, let me go over several caveats related to
        my data set,
        some practical, and one ethical.
        <br><br>
        First, web-scraping is fiendishly finicky.  When the HTML isn't consistent or gets changed, the
        whole sections of the program must be debugged and rewritten.  Thankfully, during the time I have been working
        on this program as a hobby, this hasn't been a problem.  But, there are still some users
        who slip through the cracks so to speak.  For whatever reason,
        their profiles have strange and unique HTML.  However I see no reason to think that this poses a
        statistical threat,
        only that we have dropped random observations.
        <br><br>
        Also, CouchSurfing groups hosts by city, but the profile of a user in any given city will contain
        information about experiences in many other places in addition to that city.  However, it is
        quite a task to filter and sort experiences by city, although this would of course be interesting.
        For our task, to answer our question, we will not be able to say exactly how geography
        influences things, as we have collected our sample in a manner that only roughly represent the
        geographical location from which it was collected.
        <br><br>
        I don't wish to delve into my findings here, but it must be pointed out that there
        are very few negative experiences.  This is wonderful.  But from a statistical standpoint,
        a bit of a complication; we must ensure that we have enough observations.  In my data set from Helsinki
        there were just over 50 negative or neutral experiences.  As a check, I have collected the entire reviews from
        these interactions, and once again, wonderfully, they are duller than one would expect.  However,
        this further shrinks our sample of negative experiences.  For example, people often mark as negative something
        to the effect of, "Joe was super nice, thanks."  By including these seemingly innocuous reviews as
        negative experiences I am making the implicit assumption that, on average, people
        purposefully marked the review
        as negative and for whatever reason didn't elaborate.
        <br><br>
        Furthermore, one might wish to be
        careful about separating hosts from surfers.  This is another dimension that for our purpose
        is a needless complication.
        <br><br>
        Also, I would like to assure reader that I have scoured the terms and conditions of CouchSurfing and
        am certain our inquiry into safety is anything but a violation.  In fact, the amount of
        wonderful experiences and very very few negative is a confirmation to the genius of
        the idea.  That being said,
        I will not post my web-scraper directly, mostly because it is not production ready so to speak;
        it could well break if CouchSurfing changes any small bit of HTML.  However, I would be happy
        to help anyone with a worthy cause who would like to tap into the wealth of information available online.
        We live in an amazing, and not a small bit frightening, time when so much information is
        available to those who know how to collect and manipulate it.
      </p>
</div>

<div class="paper-section">
    <h3>III. The Model and Algorithm</h3>
      <p class="paper-text">
        To understand the factors that affect whether or not one is likely to have a good
        experience on CoushSurfing, one really only needs a binary classification method.
        There are many to choose from, and most have their analogous variant within the Bayesian
        framework.  As a logical place to start,
        it would be ideal to fit a linear model.  We can do this quite elegantly by augmenting our
        data set using a Gibbs sampler.  Then, the rest is simply collecting our posterior
        draws to learn about their distribution and following up by checking how sensitive
        this is to our prior assumptions.
        <br><br>
        We could of course fit a linear model without fuss, however, there is the not-insurmountable
        problem that our dependent variable is binary, hence a linear model without corrections
        is not going to have desirable properties.  Suppose for a moment, however, that we were
        not predicting yes or no, but rather some linear variable with a cutoff that values above which
        correspond to 1, and to 0 for below.  Such a variable, in a more general context, corresponds
        to the utility of a decision.  Specifically, in our context, it is rather odd
        to speak of the utility of choosing to have a good experience, but this only says that
        there are situation, i.e. user profiles, that make it more or less likely that everyone
        will have a nice time.  Accepting this latent variable, we could then follow the standard Bayesian procedures
        and calculate our posteriors from known distributions.  But we are not given this variable, but rather
        we must infer it by continuous updating based on repeated sampling within a Gibbs sampler.  Allow
        me to elaborate on the procedure outlined in Koop.<sup>2</sup>  I also hope that all that
        follows in this section should be clear to a reader acquainted with Bayesian theory as well
        as those who are not - the details and formulae are easily found in my sources.
        <br><br>
        As the analysis takes place in a Bayesian framework, it follows that we should say something about
        our priors.  For a model such as ours, there is no reason to make complicating assumptions regarding
        our prior distributions; we stick with the standard ones.  Also, there is no great reason to use
        highly informative priors other than based on my experience and, frankly, biases.  I hope the readers
        can appreciate the metaphor of how using too strong a prior belief has the potential not only to
        influence our conclusions, but what could otherwise be wonderful experiences.
        <br><br>
        Based on <a href="#" data-toggle="tooltip" data-placement="bottom" title="Namely, that the
        disribution of our errors is normal and that they are independent of elements of X, and our
        prior beliefs are that &beta; is distributed
        multivariate normal
        and h is distributed Gamma.  These lead to the all the equations implicit in what follow as well
        as my code.">rather vanilla
        assumptions within the Bayesian Theory</a>
        , we know the distribution for our dependent
        and continuous variable (that we don't yet know) given the parameters, &beta;, is normal.  This
        distribution is built of two truncated halves, hence the indicator function.  When y is 1, we take
        right half of our normal distribution, and when y is 0, the left.  This is actually quite intuitive and
        reflects that we
        are setting a dividing line and capturing the fact that
        some observations have characteristics that
        suggest one binary option, some suggest the other.  Our task is to build this variable so we can
        estimate our parameters of interest.
        <br><br>
      </p>
      <div>
          <div class="equation">
            p( y*<sub>i</sub> | y<sub>i</sub>, &beta;, h ) ~
            N( x'<sub>i</sub>&beta; , h<sup>-1</sup> ) Indicator(y*<sub>i</sub>)
          </div>
          <div class="equation-number">
            ( 3.1 )
          </div>
      </div>
      <br><br>
      <p class="paper-text">
        Furthermore, standard theory tells us that our posterior is Normal-Gamma, note that I leave
        the parameters out,
        what we care about is that they depend on constants that are calculated from y*.
      </p>
      <br><br>
      <div>
          <div class="equation">
            p( &beta;, h | y* ) ~ NG( &middot; )
          </div>
          <div class="equation-number">
            ( 3.2 )
          </div>
      </div>
      <br><br>
      <p class="paper-text">
        The problem is the interdependence; we can't know one without the other,
        and we know neither.  But, this circular impossibility leads to a rather elegant solution.
        Suppose we are make a guess for an initial value for our continuous variable, y*, then
        we could easily calculate
        the distribution of our posteriors by sampling from (3.2).  But now that we know our posterior parameters,
        we can make a best guess at a new value for y* by sampling from (3.1).  This isn't so strait forward.  We
        sample in the following way.  For each observation, calculate x'<sub>i</sub>&beta; and make a draw from
        (3.1), if this value is positive, and corresponds to a y value of 1, then keep this as our new value for y*.  However
        if our draw is negative while corresponding to y=1, we must throw away the draw and keep the old y*.  This
        is because we know a priori that such a value isn't possible, recall that we are building up a distribution
        from two truncated halves.  Similarly, for observations where y=0, we only keep negative draws, and
        save the old values if the new draw isn't suitable.
        This process is a convergent markov chain<sup>3</sup>, and after sufficiently many simulations, we
        will have stable distributions for our posteriors as well as y*.  Of course, one must check how many
        simulation rounds are sufficient, we will cover this soon.
        <br><br>
        One might be worried that since our data set is uneven, that is, there are far more
        good experiences than bad, that by building up our latent variable by using 0 as our truncation
        point we are creating a lop-sided normal distribution centered at 0.  This is not
        the case.  Indeed our y* will be normal,
        it will simple be shifted since the bulk of the observations will be positive and correspond
        to the 1's in y.  Recall, we are drawing y*'s from a distribution centered around x<sub>i</sub>&beta;,
        not 0.  Comparing to 0 is simply a check that we have drawn a feasible new guess for y*.
        <br><br>
        There is one more detail before we are ready to implement the algorithm - identification.  There are
        infinitely many parameter values that lead to the same model, that is, we are building up a normal
        distribution, but there is nothing that constrains its variance.  Without pinning this down, we can
        not say anything about our parameters.  However, we can fix h to 1, which then ensures a unique estimate
        for &beta; which serves to answer our question.
        <br><br>
        To see how things work, I first work with generated data.  Below is
        sample output of our algorithm for 100 simulations based on the relationship
        Y = 3X + 10Z + &epsilon;.  One can also follow the link to change certain
        parameters and explore their effect on our sampler and its convergence to the posterior.
      </p>
    <h3 class="paper-text"><a href="{% url 'probit_input' %}">The algorithm</a></h3>
      <p class="paper-text">
        Despite the nature of this data, there are many salient features worth noting.  First, if our
        prior belief is something like [0,0],[1,1], our algorithm will converge, but not to the correct
        coefficients because our prior's are restricting the algorithm from iterating where it should.  We
        can avoid this problem by making our variance prior much larger, such as [200,200], but then, one
        notices that our algorithm wanders around the correct coefficient, but our convergence diagnostics
        is not satisfied.  Suppose we knew our priors, and set them to [3,10], then
        with a low prior on variance of say [.1,.1], we would converge rapidly, and furthermore have a tighter
        posterior density.  Also, reassuringly, it seems to be very difficult to get a result of the
        wrong sign, for example, starting off with a negative prior belief on our betas will not result
        in a negative posterior mean.  All in all, the point is, one must be exceedingly careful when using this method with real data,
        and we will according set our priors in what follows.
        <br><br>
        For a more dynamic look at exactly how our priors influence the posterior distributions, one may
        manipulate the tool in the following link.
      </p>
    <h3 class="paper-text"><a href="{% url 'sensitivity' %}">Sensitivity analysis</a></h3>
</div>

<div class="paper-section">
    <h3>IV. Results</h3>
      <p class="paper-text">
        Now we have our assumptions, data and the algorithm, it is time to see what can be learned about our main
        inquiry - how can we increase the probability of having a good interaction with those we meet online.  The
        following links are analogous to the preceding two, but run using the actual data.  One must be careful
        in interpreting the values of our betas as we are working with a probit model.  But the sign can be
        relied on.  For example, we see that most of the density on beta 4, a dummy for males, is around -0.2,
        and we can conclude that being male increases the chances of seeing a negative experience on that profile, we
        just can't say by how much without more calculations.
      </p>
      <h3 class="paper-text"><a href="{% url 'actual' %}">The algorithm and results from our data</a></h3>
      <br>
      <p class="paper-text">
        As before, one may take a look how our priors have power to skew the results.  I found that my results are
        data driven and the prior does not influence things dramatically, or put another way, the results
        are robust.  This is because there are over a thousand observations, as compared to the 100 observations
        I use in the generated data to speed things up.
      </p>
      <h3 class="paper-text"><a href="{% url 'sensitivity_actual' %}">Sensitivity analysis</a></h3>
      <p class="paper-text">
        Above, we see that age, profile length, how long a user has been a member and male all are related
        negatively with having a good experience.  To be truly precise, one could compare a fictitious
        profile with another and using associated data for those two, and betas above, one could arrive
        at a y* value.  Recall that positive y*'s are for those users with only good experiences.  In
        this way, we can construct a more interpretable way to analyze marginal effects.  However, I have
        elected to forgo this step as I hope no one would seriously use this model - it is not yet close
        to a substitute for intuition.
      </p>
</div>

<div class="paper-section">
    <h3>V. Conclusions</h3>
      <h4></h4>
        <p class="paper-text">
          I would like to take this opportunity to say that these results are at best motivation to look
          deeper.  That is, the negative coefficients on age, and how long a user has been a member seems deceptive.
          I suspect it is not that the elderly or long-time couchsurfers are particularly pernicious, but that
          this simply reflects the fact that they, in their career, have inevitably come into contact with
          some of the stranger dwellers of this planet (this suggest including squared terms).  I am a bit puzzled
           why those with longer profiles are
          to be suspected according to our model, maybe this is a sign of craziness when one is too eager to
          describe oneself.  Or maybe there is something else going on here.  And finally, perhaps men are not
          as problematic online as stereotypes would have it, but that they are more willing to write an
          honest review, that is, to say, 'look, we didn't get along'.  There are many other stories that
          could explain what is going on.  The bottom line is, while our estimates are significant, they should not
          be compelling enough to act on.  But we have learned that there is information in such data, and
          with a little more effect, this information could become a more convincing model.
        </p>
      <h4 class="paper-text">Future research?</h4>
        <p class="paper-text">
          These results leave many options open
          for future inquiry.  The model itself is too simple, for
          example, I am certain that we are concluding that people who have been members longer are more likely
          to have had a negative experience simply because they have been such active contributors to
          the community - inevitably with time one meets some strange people.  So perhaps some squared terms
          could help.  Also, the data is too
          simple; one would need to collect much more, and take into account other variables to get a true
          picture.  I would even recommend to collect the text and analyze it using a bag of words model.  I
          have been waiting to point out as well, that CouchSurfing has such a good reputation because it, more than
          and other services I would argue, is based on a community.  Tinder, for example, doesn't have the same
          sort of structure that allows people to so fully develop their profile.  Clearly, there
          are many directions one could go in and dramatically increase the power of the results.
        </p>
</div>

<div class="paper-section">
  <h3>VI. Appendix</h3>
    <div class="paper-text">
        <h4>References</h4>
          <ol>
            <li><a target="_blank" href="https://www.minneapolisfed.org/research/sr/sr170.pdf">
              Geweke, Keane, Runkle. May 1994. Alternative Computational Approaches to
              Inference in the Multinomial Probit Model. Federal Reserve Bank of Minneapolis.</a>
            </li>
            <li>Koop. 2006. Bayesian Econometrics.</li>
            <li><a target="_blank" href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176325750">
              Tierney. The Annals of Statistics, 1994, volume 22, No. 4, 1701-1762.</a>
            </li>
          </ol>
        <h4>Source Code</h4>
          <p>
            My algorithms and data can all be found
            at my page on <a target="_blank"
            href="https://github.com/brica1000https://github.com/brica1000">GitHub</a>,
            under the elequent title of 'BayesianPS3'.
          </p>


    </div>
</div>
{% endblock %}
